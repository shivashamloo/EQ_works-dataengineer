{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "from geopy.distance import geodesic\n",
    "from math import pi, exp\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def cleanup(datasample):\n",
    "    \"\"\"\n",
    "    1 - Cleanup\n",
    "    Get DataSample\n",
    "    Return Dataframe without the suspicious records\n",
    "    \"\"\"\n",
    "    data = spark.read.options(\n",
    "        header='True',\n",
    "        inferSchema='True',\n",
    "        delimiter=',',\n",
    "    ).csv(os.path.expanduser(datasample))\n",
    "\n",
    "    # add a column that counts the number of occurance of a record based on timeSt, Latitude and Longitude.\n",
    "    data_clean = data.withColumn('repeat', f.count('*').over(Window.partitionBy([' timeSt', 'Latitude', 'Longitude'])))\n",
    "    # Delete the requests that occured more than once.\n",
    "    data_clean = data_clean.filter(data_clean['repeat'] == 1)\n",
    "    # Delete the column that saved the occurance of a request\n",
    "    data_clean = data_clean.drop('repeat')\n",
    "    return data_clean\n",
    "\n",
    "\n",
    "def dist(lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "    Calculates the distance of two points using Geodesic Distance.\n",
    "\n",
    "    Parameters:\n",
    "    lat1: Latitude of point 1\n",
    "    long1: Longitude of point 1\n",
    "    lat2: Latitude of point 2\n",
    "    long2: Longitude of point 2\n",
    "    Return distance\n",
    "    \"\"\"\n",
    "    return geodesic((lat1, long1), (lat2, long2)).kilometers\n",
    "\n",
    "\n",
    "def label(datasample, poilist):\n",
    "    \"\"\"\n",
    "    2-Label\n",
    "    Labeling each request with the closest POI.\n",
    "    Parameters:\n",
    "    datasample: Dataframe of requests without the suspicious records\n",
    "    poilist: POIList\n",
    "    return Dataframe of labeled records\n",
    "    \"\"\"\n",
    "    points = spark.read.options(\n",
    "        header='True',\n",
    "        inferSchema='True',\n",
    "        delimiter=',',\n",
    "    ).csv(os.path.expanduser(poilist))\n",
    "\n",
    "    points = points.withColumnRenamed(' Latitude', 'point_lat').withColumnRenamed('Longitude', 'point_long')\n",
    "    data_points = datasample.crossJoin(points)\n",
    "\n",
    "    distance = f.udf(dist)\n",
    "    # Add a column to save the distance between each request to the POIs.\n",
    "    data_points = data_points.withColumn('distance', distance(data_points['Latitude'], data_points['Longitude'], \\\n",
    "                                                              data_points['point_Lat'], data_points['point_Long']))\n",
    "    # Find the colsest POI for each request.\n",
    "    data_points = data_points.withColumn('POI_distance', f.min('distance').over(Window.partitionBy('_ID')))\n",
    "    # Label each request with the closest POI\n",
    "    # If there are two POI closest, keep both of the labels untill further information\n",
    "    data_points = data_points.filter(data_points['distance'] == data_points['POI_distance'])\n",
    "    return data_points\n",
    "\n",
    "\n",
    "def analysis(data_points):\n",
    "    \"\"\"\n",
    "    3-Analysis\n",
    "    Analyising the requests assigned to each POI\n",
    "    Parameters:\n",
    "    data_points: Dataframe of labeled requests\n",
    "    Returns Dataframe of average, standard deviation, radius and density of requests assigned to each POI\n",
    "    \"\"\"\n",
    "    points_analysis = data_points.groupBy('POIID').agg(f.avg('distance').alias('average'), \\\n",
    "                                                       f.stddev('distance').alias('standard deviation'), \\\n",
    "                                                       f.max('distance').alias('radius'), \\\n",
    "                                                       f.count('distance').alias('requests'))\n",
    "\n",
    "    points_analysis = points_analysis.withColumn('density',\n",
    "                                                 points_analysis['requests'] / (points_analysis['radius'] ** 2) * pi)\n",
    "    return points_analysis\n",
    "\n",
    "\n",
    "def tan(average, count, std):\n",
    "    \"\"\"\n",
    "    Calculates Hyperbolic tangent based on the input\n",
    "    parameters:\n",
    "    average: average distance between the POI to each of its assigned requests\n",
    "    Count: number of assigned requests to each POI\n",
    "    std: standard deviation of the distance between the POI to each of its assigned requests\n",
    "    Returns a value between -10 and 10\n",
    "    \"\"\"\n",
    "\n",
    "    x = 1 - (average / std) + (count / 24820)\n",
    "    return 10 * (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "\n",
    "def model_popularity(points_analysis):\n",
    "    \"\"\"\n",
    "    4a. Model\n",
    "    Calculates the popularity of a POI based on the tan function defined above\n",
    "    parameters:\n",
    "    points_analysis: Dataframe of average, standard deviation, radius and density of requests assigned to each POI\n",
    "    Returns the input dataframe with addition of one column that consists the popularity of each POI\n",
    "    \"\"\"\n",
    "    tan_h = f.udf(tan)\n",
    "\n",
    "    points_analysis = points_analysis.withColumn('popularity',\n",
    "                                                 tan_h(points_analysis['average'], points_analysis['requests'],\n",
    "                                                       points_analysis['standard deviation']))\n",
    "    return points_analysis\n",
    "\n",
    "\n",
    "def pipeline_dependency(relations_input, tasks_input, question_input):\n",
    "    \"\"\"\n",
    "    4b. Pipeline Dependency\n",
    "    Determines the path of tasks that the pipeline should take\n",
    "    parameters:\n",
    "    relations_input: input file of relations\n",
    "    tasks_input: input file of task ids\n",
    "    question_input: input file of start and goal task\n",
    "    Returns the path of required task from start to goal\n",
    "    \"\"\"\n",
    "    relations = pd.read_csv(relations_input, sep='->', header=None, names=['head', 'tail'], engine='python')\n",
    "    relations = relations.groupby('tail')['head'].apply(list)\n",
    "\n",
    "    tasks = {}\n",
    "    with open(tasks_input, mode='r') as file:\n",
    "        ids = file.readline().split(',')\n",
    "        for i in ids:\n",
    "            tasks[int(i)] = False\n",
    "\n",
    "    with open(question_input, mode='r') as file:\n",
    "        start = int(file.readline().split(':')[1].strip())\n",
    "        end = int(file.readline().split(':')[1].strip())\n",
    "\n",
    "    relations = relations.to_dict()\n",
    "\n",
    "    tasks = mark_complete(relations, start, tasks)\n",
    "    return bfs(relations, start, end, tasks)\n",
    "\n",
    "\n",
    "def mark_complete(graph, root, ids):\n",
    "    \"\"\"\n",
    "    mark the tasks that where required to be completed for the start task\n",
    "    parameters:\n",
    "    graph: dictionary of relations where each key is a task and its value is the array of its prequisition tasks\n",
    "    root: Start task\n",
    "    ids: All of the tasks assuming that none are completed\n",
    "    Returns all of the tasks with the information of whether they are completed or not\n",
    "    \"\"\"\n",
    "    queue = [root]\n",
    "    ids[root] = True\n",
    "    while queue:\n",
    "        s = queue.pop(0)\n",
    "        if s in graph.keys():\n",
    "            for node in graph[s]:\n",
    "                if not ids[node]:\n",
    "                    queue.append(node)\n",
    "                ids[node] = True\n",
    "    return ids\n",
    "\n",
    "\n",
    "def bfs(graph, root, leaf, ids):\n",
    "    \"\"\"\n",
    "    Finding the best path between two tasks using Breadth First Search\n",
    "    parameters:\n",
    "        graph: dictionary of relations where each key is a task and its value is the array of its prequisition tasks\n",
    "    root: Start task\n",
    "    leaf: goal task\n",
    "    ids: All of the tasks and their state of completion\n",
    "    \"\"\"\n",
    "    queue = [leaf]\n",
    "    ids[leaf] = True\n",
    "    path = []\n",
    "    while queue:\n",
    "        s = queue.pop(0)\n",
    "        path.append(s)\n",
    "        if s in graph.keys():\n",
    "            for node in graph[s]:\n",
    "                if not ids[node]:\n",
    "                    queue.append(node)\n",
    "                if ids[node]:\n",
    "                    if node not in path:\n",
    "                        path.append(root)\n",
    "                ids[node] = True\n",
    "\n",
    "    return path[::-1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.master('local').getOrCreate()\n",
    "\n",
    "    data_clean = cleanup('data/DataSample.csv')\n",
    "\n",
    "    data_points = label(data_clean, 'data/POIList.csv')\n",
    "\n",
    "    points_analysis = analysis(data_points)\n",
    "\n",
    "    popularity = model_popularity(points_analysis)\n",
    "\n",
    "    pipeline = pipeline_dependency('relations.txt', 'task_ids.txt', 'question.txt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
